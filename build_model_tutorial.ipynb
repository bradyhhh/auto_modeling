{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "### Set Environment\n",
    "import sources\n",
    "\n",
    "### Prepare Data\n",
    "**Goal: get the dataset and directories ready**\n",
    "\n",
    "- load data\n",
    "- split data into scorecards (DataSplitter)\n",
    "- create directory (DirectoryManager)\n",
    "- create Datasets (Dataset)\n",
    "\n",
    "### Select Features\n",
    "**Goal: get a short list of features**\n",
    "\n",
    "- limit range (Rangelimiter)\n",
    "- analyze features \n",
    "    - bin data (Binner)\n",
    "    - feature profiling (XProfiler)\n",
    "- reload data with selected features\n",
    "    - create customized schema in excel csv\n",
    "\n",
    "### Transform Data\n",
    "**Goal: feature engineering**\n",
    "\n",
    "- limit range (Rangelimiter)\n",
    "- bin data (Binner)\n",
    "- impute features (Imputer)\n",
    "- label WOE value (LabelTransformer)\n",
    "- save proprocessors (Pipeline)\n",
    "- *Load & Transform Additional Dataset (if available)*\n",
    "\n",
    "### Build Models with hyperparameters\n",
    "- set parameters\n",
    "- fit a model (Modeler)\n",
    "- compare model performance (ModelerRanker)\n",
    "\n",
    "### Doucument Model & Create Scoring Function\n",
    "- document data preprocessing steps\n",
    "- create scoring functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RUN:** import required modules / classes and initialize jupyter notebook environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "#os.chdir('../..')\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "from sklearn.model_selection._search import ParameterGrid\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sources.data_set import DataSet \n",
    "from sources.data_splitter import DataSplitter    \n",
    "from sources.data_transformation import RangeLimiter\n",
    "from sources.imputer import Imputer\n",
    "from sources.feature_profiling import XProfiler\n",
    "from sources.modeler import Modeler, ModelerRanker\n",
    "from sources.label_transformer import LabelTransformer \n",
    "from sources.file_controller import FileController\n",
    "from sources.console_settings import ConsoleSettings\n",
    "from sources.binner import Binner \n",
    "from sources.feature_profiling import LiftPlotter  \n",
    "from sources.directory_manager import DirectoryManager\n",
    "from sources.analyzer import Analyzer\n",
    "\n",
    "if __name__ =='__main__': \n",
    "\n",
    "    settings = ConsoleSettings()\n",
    "    settings.set_all()\n",
    "    __spec__ = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RUN:** load modeling & validation dataframes from csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    m_filepath = 'Data\\\\MODEL_PERSON_T.csv'\n",
    "    v_filepath = 'Data\\\\MODEL_PERSON_V.csv'    \n",
    "\n",
    "    m_df = pd.read_csv(m_filepath, encoding='big5')\n",
    "    v_df = pd.read_csv(v_filepath, encoding='big5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OUTPUT:** modeling_dataframe, validation_dataframe\n",
    "\n",
    "**CHECK POINT:** `number_of_samples` should align with row counts, whereas `number_of_features` aligns with feature counts in the original data. \n",
    "- ``modeling_dataframe.shape``: (number_of_samples, number_of_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(m_df.shape)\n",
    "    print(v_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Dataset into Scorecards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DataSplitter` can be used to split dataframe objects into different dataframes and store in a dictionary with the according name of the scorecard. \n",
    "\n",
    "**FIT**\n",
    "\n",
    "There are two fitting methods to choose from: `fit_numeric_rules` or `fit_nominal_rules`, with similar structure:\n",
    "- `feature`: the feature used to segment the data.\n",
    "- `criteria`: the boundry, or the assigned value of `feature` in each scorecard. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FIT-numeric:**\n",
    "\n",
    "`fit_numeric_rules(feature, criteria)`\n",
    "    \n",
    "- `criteria`\n",
    "    - Type: `dict` \n",
    "    - Format: {`str`(card_name): `int`(maximum feature value)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    data_splitter = DataSplitter()\n",
    "    data_splitter.fit_numeric_rules('HOL_P09_0240', {'low': 0, \n",
    "                                                     'medium': 500000,\n",
    "                                                     'high':'else'\n",
    "                                                    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FIT-nominal:**\n",
    "\n",
    "`fit_nominal_rules(feature, criteria)`\n",
    "    \n",
    "- `criteria`\n",
    "    - Type: `dict` \n",
    "    - Format: {`str`(card_name): `str` or `list`(assigned feature value)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    data_splitter = DataSplitter()\n",
    "    data_splitter.fit_nominal_rules('L1_PATH_NEW', {'unsec_both':['UNSEC-BOTH','UNSEC-BOTH-SEC'],\n",
    "                                                'cc':['CC_TXN','UNSEC-CC'],\n",
    "                                                'no_cc_ln':'NO_CC_LN',\n",
    "                                                'unsec':'else'\n",
    "                                                })        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRANSFORM:** apply the rules to split data.\n",
    "\n",
    "`transform(dataframe)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    m_df_dict = data_splitter.transform(m_df)\n",
    "    v_df_dict = data_splitter.transform(v_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OUTPUT:** modeling_dataframe_dictionary, validation_dataframe_dictionary\n",
    "\n",
    "**CHECKPOINT:** \n",
    "\n",
    "`number_of_samples` represents the sample counts in each scorecard, where `number_of_features` should equal to (original feature counts +1)\n",
    "- card_name: (`number_of_samples in the scorecard`, `number_of_features`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print('===== Modeling =====')\n",
    "    for scorecard in m_df_dict.keys():\n",
    "        print(scorecard, \":\", m_df_dict[scorecard].shape)\n",
    "        \n",
    "    print('===== Validation =====')\n",
    "    for scorecard in v_df_dict.keys():\n",
    "        print(scorecard, \":\", v_df_dict[scorecard].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SAVE:** save `DataSplitter` object for future scoring. Execute after directory was built. (See code below after \"Create Directories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Directory\n",
    "\n",
    "to store objects that will be generated in the following steps.\n",
    "\n",
    "**UNDER \\Models\\:**\n",
    "objects for future scoring, including:\n",
    "- data_schema \n",
    "- data_preprocessors:`DataSplitter`,`RangeLimiter`, `binner`, `NominalImputer`,`NumericImputer`, `LabelTransformer` \n",
    "\n",
    "**UNDER \\Docs\\:**\n",
    "documents for performance review and model documentation, including:\n",
    "- charts after binning (pdf)\n",
    "- documentation of binning rules (csv)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DirectoryManager` has 2 functions:\n",
    "- `make_directory(path, option)`:\n",
    "    - `path`: can be absolute path or relative path. Error will be raised if using the path that does not exist. \n",
    "    - `option`: \n",
    "        - `replace`: If the directory already exists, clean up all the contents in the directory.\n",
    "        - `pass`: Do nothing to the existed directory.\n",
    "- `make_multiple_directories(path_list, option)`:\n",
    "    - `path_list`: `list` of paths.\n",
    "    - `option`: same as `make_directory`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RUN:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    dir_manager = DirectoryManager()\n",
    "    dir_manager.make_directory('Models', option='pass')\n",
    "    dir_manager.make_directory('Docs', option='pass')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SAVE:** save `DataSplitter` object to \\Models for future scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    file_controller = FileController(filepath=\"Models\\DataSplitter.pkl\")\n",
    "    file_controller.save(data_splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataSets and Directories for each Scorecard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, turn dataframe objects into our self-defined object: `DataSet`. `DataSet` is the main object type we will use in feature selection process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    schema_path = 'Data\\\\Data_Schema.csv'\n",
    "    card_name = 'unsec_both'\n",
    "    \n",
    "    # create scorecard directory to store scorecard-specific objects\n",
    "    dir_manager.make_directory('Models\\\\{}'.format(card_name), option='replace')\n",
    "    dir_manager.make_directory('Docs\\\\{}'.format(card_name), option='replace')\n",
    "\n",
    "    modeling_scorecard_df = m_df_dict.get(card_name)\n",
    "    validation_scorecard_df = v_df_dict.get(card_name)\n",
    "    \n",
    "    modeling_dataset = DataSet()\n",
    "    modeling_dataset.load_dataset_from_df(modeling_scorecard_df, schema_filepath=schema_path)\n",
    "\n",
    "    validation_dataset = DataSet()\n",
    "    validation_dataset.load_dataset_from_df(validation_scorecard_df, schema_filepath=schema_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    modeling_dataset.x.shape\n",
    "    modeling_dataset.y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    validation_dataset.x.shape\n",
    "    validation_dataset.y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANALYSIS:** check Y% for modeling & validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(modeling_dataset.y['BAD_1Y'].sum()/modeling_dataset.y['BAD_1Y'].count())\n",
    "    print(validation_dataset.y['BAD_1Y'].sum()/validation_dataset.y['BAD_1Y'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limit Range for Each Feature\n",
    "\n",
    "To avoid outliers influnce the results of feature analysis, limit the range of each feature first. \n",
    "\n",
    "`RangeLimiter` has functions: \n",
    "- `create_range_limits`\n",
    "- `tranform`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FIT:** uses Winsorizing methodology to set upper / lower bounds according to feature values\n",
    "\n",
    "`create_range_limits(dataset, method)`: `method` has 3 options \n",
    "- `quantile_range`: Upper = q3 + 1.5*iqr , Lower = q1 - 1.5*iqr        \n",
    "- `standard_deviation`: Upper = mean + 3*standard_deviation , Lower = mean - 3*standard_deviation \n",
    "- `auto`: Uses `quantile_range` as default, but if q3-q1=0, then use `standard_deviation`\n",
    "\n",
    "`set_transform_features(exclude)`\n",
    "- `exclude`:the features in this list will not be transformed when applying `transform` to `DataSet`. \n",
    "\n",
    "Reference Site: https://heartbeat.fritz.ai/how-to-make-your-machine-learning-models-robust-to-outliers-44d404067d07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    range_limiter = RangeLimiter() \n",
    "    range_limiter.create_range_limits(modeling_dataset, method='auto') #options: quantile_range, standard_deviation, auto\n",
    "    #range_limiter.set_transform_features(exclude=['HOL_P09_10', 'HOL_P09_11'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRANSFORM:** limit ranges for each feature according to the fitted upper / lower bounds\n",
    "\n",
    "`transform(dataset)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    modeling_dataset = range_limiter.transform(modeling_dataset)\n",
    "    validation_dataset = range_limiter.transform(validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OUTPUT:** `modeling_dataset`, `validation_dataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CHECKPOINT:** each feature's `min` / `max` should be within its upper / lower bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # fitted upper/lower bounds in the range limiter\n",
    "    print(range_limiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(modeling_dataset.x.describe())\n",
    "    print(validation_dataset.x.describe())\n",
    "\n",
    "#    print(modeling_dataset.x.max())\n",
    "#    print(modeling_dataset.x.min())\n",
    "#    print(validation_dataset.x.max())\n",
    "#    print(validation_dataset.x.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binner:  analyze each feature's lift\n",
    "bins the original data and check lift to select features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Binner` include several functions: \n",
    "- **FIT:** creates bins & upper_bounds for each feature according to specified methods\n",
    "- **TRANSFORM:** transforms data according to fit result\n",
    "- **PLOT:** plots lift charts for each feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FIT-1:**\n",
    "\n",
    "`fit(dataset, nominal_rules, numeric_rules)`\n",
    "\n",
    "- `nominal_rules`: `dict` in format:`{'method':'order', 'criteria':['event_rate(%)','desc']}`\n",
    "    - `method`: how to create bins, there is 1 option\n",
    "        - `order`: order fields according to `criteria`\n",
    "        - `manual`: allows manual-defined  binning rules\n",
    "    - `criteria`: `list` in format `[criteria, ordering]` \n",
    "        - `criteria`: which criteria follow. There are 2 options: `woe` or `event_rate(%)`\n",
    "        - `ordering`: `asc` for ascending or `desc` for descending\n",
    "\n",
    "- `numeric_rules`: `dict` in format: `{'max_bins':10, 'method':'percentile', 'auto': 'merge', 'criteria':'woe'}`\n",
    "    - `max_bins`: the maximum bin count\n",
    "    - `method`: how to create bins, there are 3 options\n",
    "        - `percentile`: split evenly in sample percentiles. Ex: if bin_count=10, creates decile bins\n",
    "        - `range`: split evenly in feature range. Ex: if feature ranges from 1~10 and bin_count=10, bins=(-inf,1], (1,2], ...(9,10]\n",
    "        - `tree`: split with DecisionTree algorithm, where max_leaves_count=bin_count\n",
    "        - `manual`: allows manual-defined  binning rules\n",
    "    - `auto`: whether or not to auto-bin until each bins' criteria is in monotonic order\n",
    "        - `none`: don't auto-bin\n",
    "        - `merge`: start from max_bins, if criteria is not in monotonic order, then merge the bins not in order with its previous bin until criteria reaches monotonic order \n",
    "        - `decrease`: start from max_bins, if criteria is not in monotonic order, then decrease bin_count by 1 until criteria reaches monotonic order \n",
    "    - `criteria`: which criteria to check if ordered, there are 2 options: \n",
    "        - `woe`\n",
    "        - `event_rate(%)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    method = 'percentile'\n",
    "    auto = 'merge'\n",
    "    criteria = 'woe'\n",
    "        \n",
    "    binner = Binner()\n",
    "    binner.fit(modeling_dataset, \n",
    "                   nominal_rules={'method':'order', 'criteria':['event_rate(%)','desc']}, \n",
    "                   numeric_rules={'max_bins':10, 'method':method, 'auto':auto, 'criteria':criteria})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FIT-2:**\n",
    "\n",
    "`fit_specific_features(dataset, **features_n_rules)`, in which `**feature_n_rules` specifies fitting methodology / manual assign bin boundaries for specific features, formatted as: \n",
    "\n",
    "       'HOL_P10_9': {'feature_type':'numeric',\n",
    "                       'rules':{'max_bins':10, \n",
    "                                'method':'range', \n",
    "                                'auto':'merge', \n",
    "                                'criteria':'woe'}}\n",
    "\n",
    "       'HOL_P09_9': {'feature_type':'numeric',\n",
    "                      'rules':{'max_bins':None, \n",
    "                               'method':'manual', \n",
    "                               'auto':'none', \n",
    "                               'criteria':{0:1245000, 1: 'else'}}}\n",
    "\n",
    "       'CORP_TYPE_NEW': {'feature_type':'nominal',\n",
    "                          'rules':{'method':'manual', \n",
    "                                   'criteria':{0: ['高科技電子業','穩定收入族群'], 1: 'else'}}}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    binner.fit_specific_features(modeling_dataset, \n",
    "                                 **{'HOL_P10_9': {'feature_type':'numeric',\n",
    "                                                   'rules':{'max_bins':10, \n",
    "                                                            'method':'range', \n",
    "                                                            'auto':'merge', \n",
    "                                                            'criteria':'woe'}},\n",
    "                                    'HOL_P09_9': {'feature_type':'numeric',\n",
    "                                                      'rules':{'max_bins':None, \n",
    "                                                               'method':'manual', \n",
    "                                                               'auto':'none', \n",
    "                                                               'criteria':{0:1245000, 1: 'else'}}},\n",
    "                                    'CORP_TYPE_NEW': {'feature_type':'nominal',\n",
    "                                                          'rules':{'method':'manual', \n",
    "                                                                   'criteria':{0: ['高科技電子業','穩定收入族群'], 1: 'else'}}} \n",
    "                                    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PLOT-1:**\n",
    "\n",
    "`plot_all(dataset_dict, filepath)` analyzes a single dataset & plots lift chart to pdf\n",
    "- `dataset_dict`: `dict` object under format `{data_name: dataset}` *only allows 1 dataset\n",
    "- `filepath`: plot will be auto-waved to pdf file with this filepath "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    binner.plot({'modeling':modeling_dataset}, filepath='Docs\\\\{0}\\\\Bivariable_Analysis_{1}_{2}.pdf'.format(card_name, method, auto))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`plot_multi_datasets_all(complete_modeling_dataset, complete_validation_dataset, filepath)` analyzes multiple datasets & plots lift chart to pdf\n",
    "- `complete_modeling_dataset`: `dict` object under format `{data_name: dataset}` *only 1 modeling dataset\n",
    "- `complete_validation_dataset`: `dict` object under format `{data_name: dataset, data_name2: dataset2}`, accepts 1+ validation datasets\n",
    "- `filepath`: plot will be auto-waved to pdf file with this filepath "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    binner.plot_multi_datasets(complete_modeling_dataset={'modeling':modeling_dataset},\n",
    "                               complete_validation_datasets={'validation':validation_dataset},\n",
    "                               filepath='Docs\\\\{0}\\\\MultiDataset_Bivariable_Analysis_{1}_{2}.pdf'.format(card_name, method, auto))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XProfiler:  Correlation between X's and Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANALYZE:** check correlation heat-map to decide whether to remove highly-related X's  *may take several minutes if many features \n",
    "- Check `Correlations`: `Pearson` and `Spearman` sections for heat map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    x_profiler = XProfiler()\n",
    "    x_profiler.create_report(modeling_dataset)\n",
    "    x_profiler.to_html(filepath=r'Models\\XProfileReport.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload Data with Selected Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CREATE CUSTOMIZED SCHEMA:**\n",
    "- Original data_schema csv file can be found in `Data\\` folder\n",
    "- Edit: for selected features, set Category='X'; for others, change Category to any other value (ex: 'X_drop')\n",
    "- Save it to `Models\\card_name\\` folder\n",
    "\n",
    "**RUN:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    card_name = 'unsec_both'\n",
    "    schema_path = 'Models\\\\{0}\\\\Data_Schema_{1}.csv'.format(card_name, card_name) \n",
    "    \n",
    "    modeling_scorecard_df = m_df_dict.get(card_name)\n",
    "    validation_scorecard_df = v_df_dict.get(card_name)\n",
    "    \n",
    "    modeling_dataset = DataSet()\n",
    "    modeling_dataset.load_dataset_from_df(modeling_scorecard_df, schema_filepath=schema_path)\n",
    "\n",
    "    validation_dataset = DataSet()\n",
    "    validation_dataset.load_dataset_from_df(validation_scorecard_df, schema_filepath=schema_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    modeling_dataset.x.shape\n",
    "    modeling_dataset.y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    validation_dataset.x.shape\n",
    "    validation_dataset.y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limit Range for Each Feature\n",
    "`RangeLimiter`: See `Select Feature`-`Limit Range for Each Feature` description above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FIT:** uses Winsorizing methodology to set upper / lower bounds according to feature values\n",
    "\n",
    "`create_range_limits(dataset, method)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    range_limiter = RangeLimiter() \n",
    "    range_limiter.create_range_limits(modeling_dataset, method='auto') #options: quantile_range, standard_deviation, auto\n",
    "    #range_limiter.set_transform_features(exclude=['HOL_P09_10', 'HOL_P09_11'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRANSFORM:** limit ranges for each feature according to the fitted upper / lower bounds\n",
    "\n",
    "`transform(dataset)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    modeling_dataset = range_limiter.transform(modeling_dataset)\n",
    "    validation_dataset = range_limiter.transform(validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OUTPUT:** `modeling_dataset`, `validation_dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    print(modeling_dataset.x.describe())\n",
    "    print(validation_dataset.x.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bin Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binner:  analyze each feature's lift\n",
    "bins the original data to make x-y relationships more linear & stable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Binner` include several functions: \n",
    "- **FIT:** creates bins & upper_bounds for each feature according to specified methods\n",
    "- **TRANSFORM:** transforms data according to fit result\n",
    "- **PLOT:** plots lift charts for each feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binner:  analyze each feature's lift\n",
    "bins the original data and check lift to select features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Binner` include several functions: \n",
    "- **FIT:** creates bins & upper_bounds for each feature according to specified methods\n",
    "- **TRANSFORM:** transforms data according to fit result\n",
    "- **PLOT:** plots lift charts for each feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FIT-1:**\n",
    "\n",
    "`fit(dataset, nominal_rules, numeric_rules)`\n",
    "\n",
    "- `nominal_rules`: `dict` in format:`{'method':'order', 'criteria':['event_rate(%)','desc']}`\n",
    "    - `method`: how to create bins, there is 1 option\n",
    "        - `order`: order fields according to `criteria`\n",
    "        - `manual`: allows manual-defined  binning rules\n",
    "    - `criteria`: `list` in format `[criteria, ordering]` \n",
    "        - `criteria`: which criteria follow. There are 2 options: `woe` or `event_rate(%)`\n",
    "        - `ordering`: `asc` for ascending or `desc` for descending\n",
    "\n",
    "- `numeric_rules`: `dict` in format: `{'max_bins':10, 'method':'percentile', 'auto': 'merge', 'criteria':'woe'}`\n",
    "    - `max_bins`: the maximum bin count\n",
    "    - `method`: how to create bins, there are 3 options\n",
    "        - `percentile`: split evenly in sample percentiles. Ex: if bin_count=10, creates decile bins\n",
    "        - `range`: split evenly in feature range. Ex: if feature ranges from 1~10 and bin_count=10, bins=(-inf,1], (1,2], ...(9,10]\n",
    "        - `tree`: split with DecisionTree algorithm, where max_leaves_count=bin_count\n",
    "        - `manual`: allows manual-defined  binning rules\n",
    "    - `auto`: whether or not to auto-bin until each bins' criteria is in monotonic order\n",
    "        - `none`: don't auto-bin\n",
    "        - `merge`: start from max_bins, if criteria is not in monotonic order, then merge the bins not in order with its previous bin until criteria reaches monotonic order \n",
    "        - `decrease`: start from max_bins, if criteria is not in monotonic order, then decrease bin_count by 1 until criteria reaches monotonic order \n",
    "    - `criteria`: which criteria to check if ordered, there are 2 options: \n",
    "        - `woe`\n",
    "        - `event_rate(%)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    method = 'percentile'\n",
    "    auto = 'merge'\n",
    "    criteria = 'woe'\n",
    "        \n",
    "    binner = Binner()\n",
    "    binner.fit(modeling_dataset, \n",
    "                   nominal_rules={'method':'order', 'criteria':['event_rate(%)','desc']}, \n",
    "                   numeric_rules={'max_bins':10, 'method':method, 'auto':auto, 'criteria':criteria})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FIT-2:**\n",
    "\n",
    "`fit_specific_features(dataset, **features_n_rules)`, in which `**feature_n_rules` specifies fitting methodology / manual assign bin boundaries for specific features, formatted as: \n",
    "\n",
    "       'HOL_P10_9': {'feature_type':'numeric',\n",
    "                       'rules':{'max_bins':10, \n",
    "                                'method':'range', \n",
    "                                'auto':'merge', \n",
    "                                'criteria':'woe'}}\n",
    "\n",
    "       'HOL_P09_9': {'feature_type':'numeric',\n",
    "                      'rules':{'max_bins':None, \n",
    "                               'method':'manual', \n",
    "                               'auto':'none', \n",
    "                               'criteria':{0:1245000, 1: 'else'}}}\n",
    "\n",
    "       'CORP_TYPE_NEW': {'feature_type':'nominal',\n",
    "                          'rules':{'method':'manual', \n",
    "                                   'criteria':{0: ['高科技電子業','穩定收入族群'], 1: 'else'}}} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    binner.fit_specific_features(modeling_dataset, \n",
    "                                 **{'HOL_P10_9': {'feature_type':'numeric',\n",
    "                                                   'rules':{'max_bins':10, \n",
    "                                                            'method':'range', \n",
    "                                                            'auto':'merge', \n",
    "                                                            'criteria':'woe'}},\n",
    "                                    'HOL_P10_12': {'feature_type':'numeric',\n",
    "                                                      'rules':{'max_bins':None, \n",
    "                                                               'method':'manual', \n",
    "                                                               'auto':'none', \n",
    "                                                               'criteria':{0:999, 1: 'else'}}},\n",
    "                                    'CORP_TYPE_NEW': {'feature_type':'nominal',\n",
    "                                                          'rules':{'method':'manual', \n",
    "                                                                   'criteria':{0: ['高科技電子業','穩定收入族群'], 1: 'else'}}} \n",
    "                                    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PLOT-1:**\n",
    "\n",
    "`plot(dataset_dict, filepath)` analyzes a single dataset & plots lift chart to pdf\n",
    "- `dataset_dict`: `dict` object under format `{data_name: dataset}` *only allows 1 dataset\n",
    "- `filepath`: plot will be auto-waved to pdf file with this filepath "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    binner.plot({'modeling':modeling_dataset}, filepath='Docs\\\\{0}\\\\Bivariable_Analysis_{1}_{2}.pdf'.format(card_name, method, auto))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`plot_multi_datasets_all(complete_modeling_dataset, complete_validation_dataset, filepath)` analyzes multiple datasets & plots lift chart to pdf\n",
    "- `complete_modeling_dataset`: `dict` object under format `{data_name: dataset}` *only 1 modeling dataset\n",
    "- `complete_validation_dataset`: `dict` object under format `{data_name: dataset, data_name2: dataset2}`, accepts 1+ validation datasets\n",
    "- `filepath`: plot will be auto-waved to pdf file with this filepath "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    binner.plot_multi_datasets(complete_modeling_dataset={'modeling':modeling_dataset},\n",
    "                               complete_validation_datasets={'validation':validation_dataset},\n",
    "                               filepath='Docs\\\\{0}\\\\MultiDataset_Bivariable_Analysis_{1}_{2}.pdf'.format(card_name, method, auto))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRANSFORM:** \n",
    "\n",
    "`transform(dataset)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    modeling_dataset = binner.transform(modeling_dataset)\n",
    "    validation_dataset = binner.transform(validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OUTPUT:** `modeling_dataset`, `validation_dataset`\n",
    "\n",
    "**CHECKPOINT:** X values should be binned according to fitted boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(modeling_dataset.x.head())\n",
    "    print(validation_dataset.x.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROGRESS:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(binner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FIT-1:** selects a value to replace `np.nan` with specified rules\n",
    "\n",
    "`fit(dataset, numeric_rule, numeric_value, nominal_rule, nominal_value)`\n",
    "- `nominal_rule` has 2 options: \n",
    "    - `most_frequent`: fills `np.nan` (if any) with the value with the largest samples\n",
    "    - `constant`: fills `np.nan` (if any) with string `nominal_value`\n",
    "- `nominal_value`: default `'NaN'` if not specified\n",
    "- `numeric_rule` has 2 options: \n",
    "    - `median`: fills `np.nan` (if any) with median\n",
    "    - `mean`: fills `np.nan` (if any) with mean\n",
    "    - `constant`: fills `np.nan` (if any) with `numeric_value`\n",
    "- `numeric_value`: default `0` if not specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    imputer = Imputer()\n",
    "    imputer.fit(modeling_dataset, numeric_rule=\"median\", nominal_rule=\"most_frequent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FIT-2:** assign values to be imputed manually \n",
    "\n",
    "`fit_specific_features(numeric_features, nominal_features)`\n",
    "- `numeric_features`: `dict` that contains {`feature1`:`impute_value1`, `feature2`:`impute_value2`}\n",
    "- `nominal_features`: `dict` that contains {`feature1`:`impute_value1`, `feature2`:`impute_value2`}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    imputer.fit_specific_features(numeric_features={'USE_P12_0254': 0, 'INQ_P01_0012': 2}, \n",
    "                                  nominal_features={'CORP_TYPE_NEW': 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRANSFORM:** replaces `np.nan` in nominal features with fitted values\n",
    "\n",
    "`trasnform(dataset)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    modeling_dataset = imputer.transform(modeling_dataset) \n",
    "    validation_dataset = imputer.transform(validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OUTPUT:** `modeling_dataset`, `validation_dataset`\n",
    "\n",
    "**CHECKPOINT:** all nominal features should contain no `np.nan`\n",
    "- `dataset.x.isnull().sum()` should be 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    modeling_dataset.x.isnull().sum()\n",
    "    validation_dataset.x.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROGRESS:** shows features and its respective impute values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(imputer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label binned data value as WOE values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FIT:** calculates woe value for each bin, and stores data for bin labeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    label_transformer = LabelTransformer()\n",
    "    label_transformer.fit(modeling_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRANSFORM:** labels each row with its bins' woe value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    modeling_dataset = label_transformer.transform(modeling_dataset)\n",
    "    validation_dataset = label_transformer.transform(validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OUTPUT:** `modeling_dataset`, `validation_dataset`\n",
    "\n",
    "**CHECKPOINT:** values should be assigned to woe values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    modeling_dataset.x.head()\n",
    "    validation_dataset.x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save preprocessors to Pipeline\n",
    "collect proprocessors into a `Pipeline` object and save it for future data preperation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    pipeline = Pipeline([\n",
    "            ('range_limiter',range_limiter), \n",
    "            ('binner', binner), \n",
    "            ('imputer', imputer), \n",
    "            ('label_transformer', label_transformer)\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    file_controller = FileController(\"Models\\\\{0}\\\\Pipeline.pkl\".format(card_name))\n",
    "    file_controller.save(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load & Transform Additional Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SAVE:** combine all preprocessors into one single `Pipeline` & save to pkl file for future scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRANSFORM:** follow the same steps to load & transform additional datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    v_pl_filepath = 'Data\\\\MODEL_PROD_V_PL.csv'\n",
    "    \n",
    "    v_pl_df = pd.read_csv(v_pl_filepath, encoding='big5')\n",
    "    v_pl_df_dict = data_splitter.transform(v_pl_df)\n",
    "    \n",
    "    validation_pl_scorecard_df = v_pl_df_dict.get(card_name)\n",
    "    \n",
    "    validation_pl_dataset = DataSet()\n",
    "    validation_pl_dataset.load_dataset_from_df(validation_pl_scorecard_df, schema_filepath=schema_path)\n",
    "    \n",
    "    validation_pl_dataset = pipeline.transform(validation_pl_dataset)\n",
    "\n",
    "#    validation_pl_dataset = range_limiter.transform(validation_pl_dataset)\n",
    "#    validation_pl_dataset = binner.transform_all(validation_pl_dataset)\n",
    "#    validation_pl_dataset = imputer.transform(validation_pl_dataset)\n",
    "#    validation_pl_dataset = label_transformer.transform(validation_pl_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OUTPUT:** `dataset`\n",
    "\n",
    "**CHECKPOINT:** should contain no nulls and values;  should be binned & labeled as WOE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    validation_pl_dataset.x.describe()\n",
    "    validation_pl_dataset.x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a single model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SET PARAMETER:**\n",
    "\n",
    "*Function is implemented with `sklearn.linear_model.LogisticRegression()`, for detailed parameter definitions see reference: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    model_id = 'LogisticRegression_test'\n",
    "    parameter = {'C':1.0, 'class_weight':'balanced', 'n_jobs':-1, 'random_state':297, 'tol':0.0001}\n",
    "    clf = LogisticRegression()\n",
    "    clf = clf.set_params(**parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Modeler\n",
    "`Modeler` is a customized object to train and evaluate models.\n",
    "\n",
    "There are 4 functions in `Modeler`:\n",
    "- `fit`: train the model, bin the predicted event probability and generate performance evaluation accordingly.\n",
    "- `fit_binner`: reset the binning rules for this model, and regenerate performance evaluation.\n",
    "- `plot`: plot the binning result and output to file\n",
    "\n",
    "There are 3 properties in `Modeler`:\n",
    "- `clf`: the fitted classifier (based on sklearn), presents the selected parameter\n",
    "- `performance_df`: presents model performance indicators, including KS, IV, PSI...\n",
    "- `feature_info`(for now, only applies to linear models): presents the predictors and their coefficient, correlation, p-value and correction \n",
    "\n",
    "**RUN:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    modeler = Modeler(model_id)\n",
    "    modeler.fit(clf, \n",
    "                modeling_dataset={'modeling':modeling_dataset},\n",
    "                validation_datasets={'validation_1':validation_dataset, 'validation_2':validation_dataset},\n",
    "                rules={'method':'percentile','max_bins':10, 'auto':'none','criteria':'none'})\n",
    "\n",
    "    modeler.plot(modeling_dataset={'modeling':modeling_dataset},\n",
    "                 validation_datasets={'validation_1':validation_dataset, 'validation_2':validation_pl_dataset},\n",
    "                 plot_path='Docs\\{}'.format(card_name)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VIEW MODEL INFO:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(modeler.clf)\n",
    "    print(modeler.performance_df)\n",
    "    print(modeler.feature_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SAVE:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    file_controller = FileController(filepath=\"Models\\\\{0}\\\\{1}.pkl\".format(card_name,model_id))\n",
    "    file_controller.save(modeler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Models with Hyperparameters\n",
    "**SET PARAMETERS**\n",
    "- define `param_grid`\n",
    "\n",
    "**FIT**\n",
    "- create & fit `Modeler` object for each parameters\n",
    "- collect these `Modeler` results in a `Dict` for later model selection\n",
    "\n",
    "*Function is implemented with `sklearn.linear_model.LogisticRegression()`, for detailed parameter definitionssee reference: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    param_grid = {   'penalty': ['l1','l2'] #done\n",
    "    #                    ,'dual': [True,False]   #\"dual\" does not change the result, only potentially the speed of the algorithm\n",
    "                    ,'tol': np.logspace(-5,1,7) #done\n",
    "                    ,'C': np.logspace(-3,1,5) #done\n",
    "    #                    ,'fit_intercept': [True,False] #done\n",
    "    #                    ,'intercept_scaling': [1]\n",
    "                    ,'class_weight': ['balanced',{0:0.01,1:0.99},{0:0.005,1:0.995},{0:0.001,1:0.999}]\n",
    "                    ,'random_state': [None]\n",
    "    #                    ,'solver': ['newton-cg','lbfgs','liblinear','sag','saga'] #done\n",
    "    #                    ,'max_iter': [100]\n",
    "    #                    ,'multi_class': ['ovr','multinomial']\n",
    "    #                    ,'verbose': [0]\n",
    "    #                    ,'warm_start': [True,False] #doesn't impact much \n",
    "                    ,'n_jobs': [-1]\n",
    "                 }    \n",
    "    param_combinations = list(ParameterGrid(param_grid))\n",
    "\n",
    "    n=0\n",
    "    modelers = {}\n",
    "    \n",
    "    for parameters in product(param_combinations):\n",
    "        n += 1\n",
    "        model_id = 'LogistictRegression_'+str(n)\n",
    "        clf = LogisticRegression()    \n",
    "        clf = clf.set_params(**parameters[0])\n",
    "        \n",
    "        modeler = Modeler(model_id)\n",
    "        modeler.fit(clf, \n",
    "                modeling_dataset={'modeling':modeling_dataset},\n",
    "                validation_datasets={'validation_1':validation_dataset, 'validation_2':validation_pl_dataset},\n",
    "                rules={'method':'percentile','max_bins':10, 'auto':'none','criteria':'none'})\n",
    "\n",
    "        modelers.update({model_id:modeler})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    print(modelers.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Model Performance\n",
    "Initiate a `ModelerRanker` to find the best performance model.\n",
    "\n",
    "**SET FILTER & ORDER CRITERION:** \n",
    "- `set_filters`: only models that qualify all the filter criterion will be ranked \n",
    "    - format: \n",
    "            {dataset_1: {indicator_1 : [operator, limit_value], indicator_2:[operator, limit_value]},\n",
    "            dataset_2:{indicator_1 : [operator, limit_value], indicator_2:[operator, limit_value]}}\n",
    "- `set_orders`: rank models by indicators\n",
    "    - format:\n",
    "            {priority_1:[dataset_1, indicator, ascending_order],\n",
    "            priority_2:[dataset_2, indicator, ascending_order]}\n",
    "- `rank`: apply filter and order rules to all the `modeler`s in the dictionary and output a ranking table\n",
    "\n",
    "\n",
    "**AVAILABLE INDICATORS**\n",
    "- `KS`: KS value for specific dataset, the higher the better \n",
    "- `IV`: IV value for specific dataset, the higher the better \n",
    "- `bounce_cnt`: how many times that the ordering of lift has bounced, the less the better\n",
    "- `bounce_pct`: bounce_pct% = sum(bounce_range)/total_range, the less the better \n",
    "- `r`: correlation coefficient between bin number and respond (Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    filters = {'modeling':\n",
    "                {'ks': ['>', 50],\n",
    "                 'iv':['>', 100],\n",
    "                 'bounce_cnt':['<=', 3],\n",
    "                 'bounce_pct':['<', 20],\n",
    "                 'r':['>', 0.1]}\n",
    "                ,\n",
    "               'validation_1':\n",
    "                {'ks': ['>', 40],\n",
    "                 'iv':['>', 90],\n",
    "                 'bounce_cnt':['<=', 5],\n",
    "                 'bounce_pct':['<=', 30],\n",
    "                 'r':['>=', 0.1]}                 \n",
    "               }\n",
    "                    \n",
    "    orders = { '1':['validation_1','ks','desc']\n",
    "              ,'2':['modeling','ks','desc']\n",
    "              ,'3':['validation_2','ks', 'desc']\n",
    "            }\n",
    "\n",
    "    model_ranker = ModelerRanker()        \n",
    "    model_ranker.set_filters(**filters)\n",
    "    model_ranker.set_orders(**orders)  \n",
    "    ranking_table = model_ranker.rank(modelers)      \n",
    "    print(ranking_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANALYZE:** \n",
    "\n",
    "`top_n_models` can be used to get a list of model_ids within the assigned rank. Then utilize a loop to print out the plots and compare these models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    top_n_list = model_ranker.top_n_models(n=5)\n",
    "\n",
    "    for model in top_n_list:\n",
    "        modeler = modelers.get(model)\n",
    "        print(modeler.clf)\n",
    "        print(modeler.feature_info)\n",
    "        modeler.plot(modeling_dataset={'modeling':modeling_dataset},\n",
    "                     validation_datasets={'validation_1':validation_dataset, 'validation_2':validation_pl_dataset},\n",
    "                     plot_path = 'Docs\\{}'.format(card_name))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Model & Create Scoring Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Data Preprocessing Steps "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reload Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    m_df = pd.read_csv(m_filepath, encoding='big5')\n",
    "    v_df = pd.read_csv(v_filepath, encoding='big5')\n",
    "    \n",
    "    m_df_dict = data_splitter.transform(m_df)\n",
    "    v_df_dict = data_splitter.transform(v_df)\n",
    "    \n",
    "    modeling_scorecard_df = m_df_dict.get(card_name)\n",
    "    validation_scorecard_df = v_df_dict.get(card_name)\n",
    "    \n",
    "    modeling_dataset_for_doc = DataSet()\n",
    "    modeling_dataset_for_doc.load_dataset_from_df(modeling_scorecard_df, schema_filepath=schema_path)\n",
    "    \n",
    "    validation_dataset_for_doc = DataSet()\n",
    "    validation_dataset_for_doc.load_dataset_from_df(validation_scorecard_df, schema_filepath=schema_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyze by pipeline steps**\n",
    "- Follow each step in `pipeline` and analyze process step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binner_flag = 0 \n",
    "cnt = 0 \n",
    "\n",
    "methods=['percentile', 'range']\n",
    "step = '0.0_original'\n",
    "for method in methods: \n",
    "    analyzer = Analyzer()\n",
    "    analyzer.fit(modeling_dataset_for_doc, method=method)\n",
    "    analyzer.analyze_plot_all({'modeling':modeling_dataset_for_doc}, {'validation': validation_dataset_for_doc}, dirpath='Docs\\\\{}'.format(card_name), step=step)\n",
    "\n",
    "for i in pipeline.named_steps: \n",
    "    cnt=cnt+1\n",
    "    transformer = pipeline.named_steps.get(i)\n",
    "    \n",
    "    if(i=='binner'): \n",
    "        binner_flag = 1\n",
    "        step=\"{0:.1f}_{1}\".format(cnt-0.5, \"binner\")\n",
    "        \n",
    "        analyzer = Analyzer()\n",
    "        analyzer.set_binner(transformer)\n",
    "        analyzer.analyze_plot_all({'modeling':modeling_dataset_for_doc}, {'validation': validation_dataset_for_doc}, dirpath='Docs\\\\{}'.format(card_name), step=step)\n",
    "\n",
    "    step = \"{0:.1f}_{1}\".format(cnt, i)\n",
    "    modeling_dataset_for_doc = transformer.transform(modeling_dataset_for_doc)\n",
    "    validation_dataset_for_doc = transformer.transform(validation_dataset_for_doc)\n",
    "\n",
    "    if(binner_flag==0):\n",
    "        methods = ['percentile', 'range']\n",
    "    else: \n",
    "        methods = ['original']\n",
    "        \n",
    "    for method in methods:\n",
    "        analyzer = Analyzer()\n",
    "        analyzer.fit(modeling_dataset_for_doc, method=method)\n",
    "        analyzer.analyze_plot_all({'modeling':modeling_dataset_for_doc}, {'validation': validation_dataset_for_doc}, dirpath='Docs\\\\{}'.format(card_name), step=step)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Scoring Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SAVE:** save `data_splitter`, `pipeline` (for data preprocess), best performed `modeler` to pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    file_controller = FileController(\"Models\\\\DataSplitter.pkl\")\n",
    "    file_controller.save(data_splitter)\n",
    "    \n",
    "    file_controller = FileController(\"Models\\\\{0}\\\\pipeline.pkl\".format(card_name))\n",
    "    file_controller.save(pipeline)\n",
    "    \n",
    "    model_id = 'LogistictRegression_140'\n",
    "    modeler = modelers.get(model_id)\n",
    "    file_controller = FileController(filepath=\"Models\\\\{0}\\\\Modeler.pkl\".format(card_name))\n",
    "    file_controller.save(modeler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCORE:** once files are saved to folders `Models\\`&`Models\\{card_name}\\`, `Scorer` class works automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scorer(): \n",
    "    def __init__(self, card_name=''): \n",
    "        self.card_name = card_name \n",
    "        self.schema_path = 'Models\\\\{0}\\\\Data_Schema_{0}.csv'.format(card_name)\n",
    "        \n",
    "        file_controller = FileController(\"Models\\\\DataSplitter.pkl\")\n",
    "        self.data_splitter = file_controller.load()\n",
    "        \n",
    "        file_controller = FileController(\"Models\\\\{0}\\\\Pipeline.pkl\".format(card_name))\n",
    "        self.pipeline = file_controller.load()\n",
    "\n",
    "        file_controller = FileController(filepath=\"Models\\\\{0}\\\\Modeler.pkl\".format(card_name))\n",
    "        self.modeler = file_controller.load()\n",
    "        \n",
    "    def score(self, df): \n",
    "        df_dict = self.data_splitter.transform(df)\n",
    "        scorecard_df = df_dict.get(self.card_name)\n",
    "        \n",
    "        destination_dataset = DataSet()\n",
    "        destination_dataset.load_dataset_from_df(scorecard_df, schema_filepath=self.schema_path)\n",
    "        destination_dataset = self.pipeline.transform(destination_dataset)\n",
    "        scored_dataset = self.modeler.score(destination_dataset)\n",
    "    \n",
    "        return scored_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    m_df = pd.read_csv('Data\\\\MODEL_PERSON_T.csv', encoding='big5')\n",
    "    \n",
    "    for card_name in ['unsec_both']: \n",
    "        scorer = Scorer(card_name = card_name)\n",
    "        result = scorer.score(m_df)\n",
    "        \n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VALIDATE:** validate scoring result to ensure correctness in this specific card"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
